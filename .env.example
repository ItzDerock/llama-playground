# The web-server's HOST and PORT
HOST=127.0.0.1
PORT=3000

# Disable this if you want to run llama.cpp#tcp_server on your own
# Uses the binary path set below
USE_BUILT_IN_LLAMA_SERVER=true

# Binary location for llama.cpp#tcp_server
# Auto will automatically pull and build the latest version
# Requires build-essential (or equivalent) and make
# This does nothing if USE_BUILT_IN_LLAMA_SERVER is disabled
LLAMA_TCP_BIN=auto

# If USE_BUILT_IN_LLAMA_SERVER is disabled, enter the llama.cpp#tcp_server tcp details here
# Otherwise, this app will automatically start a llama.cpp#tcp_server server
# If port is set to auto, it will listen on a random open port
LLAMA_SERVER_HOST=auto
LLAMA_SERVER_PORT=auto

# The path to a model's .bin file
LLAMA_MODEL_PATH=/path/to/llama/ggml-model-q4_0.bin